{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb37b05",
   "metadata": {},
   "source": [
    "### Import Library yang akan digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d0c1ddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d020022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0df070",
   "metadata": {},
   "source": [
    "### Memuat Data Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1d95dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tweet ID', 'Tweet Time Stamp', 'Tweet Text', 'Tweet Hashtag',\n",
       "       'Tweet Translated'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_data.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c7437",
   "metadata": {},
   "source": [
    "### Assesing dan Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d542e208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet Translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1609172529712611329</td>\n",
       "      <td>The Embark Internship has allowed our interns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1609243867504590850</td>\n",
       "      <td>asked ChatGPT: What jobs will be needed for hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1609261573184581635</td>\n",
       "      <td>Thanks to ChatGPT, I just learned a new skill!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1609043258625216512</td>\n",
       "      <td>Maybe AI won't be taking our jobs... at least,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1608679261136912384</td>\n",
       "      <td>Well, @openai #ChatGPT completely shit the bed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet ID                                   Tweet Translated\n",
       "0  1609172529712611329  The Embark Internship has allowed our interns ...\n",
       "1  1609243867504590850  asked ChatGPT: What jobs will be needed for hu...\n",
       "2  1609261573184581635  Thanks to ChatGPT, I just learned a new skill!...\n",
       "3  1609043258625216512  Maybe AI won't be taking our jobs... at least,...\n",
       "4  1608679261136912384  Well, @openai #ChatGPT completely shit the bed..."
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop kolom yang tidak di butuhkan (Timestamp, Tweet Text, Hashtag)\n",
    "df = df.drop(columns=['Tweet Time Stamp', 'Tweet Text', 'Tweet Hashtag'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0904faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Duplicated Value sebelum di drop \n",
      "False    14073\n",
      "True        33\n",
      "Name: count, dtype: int64\n",
      "Jumlah Duplicated Value setelah di drop \n",
      "False    14073\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Duplicate Value\n",
    "print(f\"Jumlah Duplicated Value sebelum di drop \\n{df.duplicated().value_counts()}\")\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Jumlah Duplicated Value setelah di drop \\n{df.duplicated().value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d873384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Null Values pada kolom Tweet ID : Tweet ID\n",
      "False    14073\n",
      "Name: count, dtype: int64\n",
      "Jumlah Null Values pada kolom Tweet Translated : Tweet Translated\n",
      "False    14073\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Null Values\n",
    "for column in df.columns:\n",
    "    print(f\"Jumlah Null Values pada kolom {column} : {df[column].isna().value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e7d2e",
   "metadata": {},
   "source": [
    "### Pre Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5f1b051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Fungsi untuk Pre processing Data\n",
    "def preprocessing(text):\n",
    "    # Inisialisasi\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    # Strip whitespaces\n",
    "    text = text.strip()\n",
    "    # Hapus URL, mentions, hashtag, newline, angka, special characters\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+|@\\w+|#\\w+|[^\\d\\w\\S]|\\d+|\\W|\\0|_+|[^\\x00-\\x7F]+\", \" \", text)\n",
    "    # Hapus punctuation\n",
    "    text = ''.join(char for char in text if char not in punctuation_set)\n",
    "    # Tokenisasi dan hapus stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    text = [lemma.lemmatize(word) for word in filtered]\n",
    "    text = \" \".join(word for word in text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c763af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet Translated</th>\n",
       "      <th>Text Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1609172529712611329</td>\n",
       "      <td>The Embark Internship has allowed our interns ...</td>\n",
       "      <td>embark internship allowed intern grow professi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1609243867504590850</td>\n",
       "      <td>asked ChatGPT: What jobs will be needed for hu...</td>\n",
       "      <td>asked chatgpt job needed human perform artific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1609261573184581635</td>\n",
       "      <td>Thanks to ChatGPT, I just learned a new skill!...</td>\n",
       "      <td>thanks chatgpt learned new skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1609043258625216512</td>\n",
       "      <td>Maybe AI won't be taking our jobs... at least,...</td>\n",
       "      <td>maybe ai taking job least yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1608679261136912384</td>\n",
       "      <td>Well, @openai #ChatGPT completely shit the bed...</td>\n",
       "      <td>well completely shit bed one pharmacologist bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1608963021824561154</td>\n",
       "      <td>“AI Rewrite: Steve Jobs Stanford Commencement”...</td>\n",
       "      <td>ai rewrite steve job stanford commencement pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1609304139116302339</td>\n",
       "      <td>🧵 How to replace bullshit jobs with AI 🧵 \\n\\n2...</td>\n",
       "      <td>replace bullshit job ai year never seen risk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1609301436428423174</td>\n",
       "      <td>Four Ways #Jobs Will Respond to #Automation \\n...</td>\n",
       "      <td>four way respond via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1608996363123007489</td>\n",
       "      <td>BGSU program to meet critical workforce needs ...</td>\n",
       "      <td>bgsu program meet critical workforce need adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1608952318787026947</td>\n",
       "      <td>New books from MIT experts deliver insights on...</td>\n",
       "      <td>new book mit expert deliver insight future rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1609185244145487873</td>\n",
       "      <td>Experts warn Artificial intelligence may repla...</td>\n",
       "      <td>expert warn artificial intelligence may replac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1609248074471464961</td>\n",
       "      <td>I explored ChatGPT today and concluded that ou...</td>\n",
       "      <td>explored chatgpt today concluded job safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1609204286981816321</td>\n",
       "      <td>Sophisticated Artificial Intelligence and mach...</td>\n",
       "      <td>sophisticated artificial intelligence machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1609039010642690048</td>\n",
       "      <td>This is priceless. Much debate on whose jobs w...</td>\n",
       "      <td>priceless much debate whose job eliminated ai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1608935714305069056</td>\n",
       "      <td>RISE OF THE ROBOTS Artificial intelligence can...</td>\n",
       "      <td>rise robot artificial intelligence replace hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1609188926295506946</td>\n",
       "      <td>3️⃣#TrendsLLYCDCH: Artificial intelligence cre...</td>\n",
       "      <td>artificial intelligence creates opportunity im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1609142479546191872</td>\n",
       "      <td>Automation &amp;amp; Artificial Intelligence is on...</td>\n",
       "      <td>automation amp artificial intelligence one dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1609112876656066560</td>\n",
       "      <td>Twitter !!!\\n\\nlooking for web3 jobs ? \\n\\nTro...</td>\n",
       "      <td>twitter looking web job tropyverse hiring digi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1609058620913905664</td>\n",
       "      <td>To save so many Jobs from vanishing we need to...</td>\n",
       "      <td>save many job vanishing need ask bjp start boy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1609232645438701569</td>\n",
       "      <td>So here is the AI Chatbot i.e. ChatGPT Intervi...</td>\n",
       "      <td>ai chatbot e chatgpt interview hand different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1609154651441565696</td>\n",
       "      <td>Does ChatGpt mean robots are coming for skille...</td>\n",
       "      <td>chatgpt mean robot coming skilled job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1609052226986860545</td>\n",
       "      <td>Register now for jobs in Data science, Machine...</td>\n",
       "      <td>register job data science machine learning art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1608945762536136705</td>\n",
       "      <td>Even though Amazon's automating various busine...</td>\n",
       "      <td>even though amazon automating various business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1609174530265677825</td>\n",
       "      <td>Onshoring the most complex factories in histor...</td>\n",
       "      <td>onshoring complex factory history require late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1609202812046479362</td>\n",
       "      <td>HIRING: Data Management Administrator I / Sain...</td>\n",
       "      <td>hiring data management administrator saint cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1609329670285041665</td>\n",
       "      <td>Does ChatGPT mean robots are coming for skille...</td>\n",
       "      <td>chatgpt mean robot coming skilled job via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1609304151137345536</td>\n",
       "      <td>🔗 How One Jobseeker Used AI to Apply for 200 J...</td>\n",
       "      <td>one jobseeker used ai apply job day ai emergin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1609084395889070080</td>\n",
       "      <td>Now This:  Google’s mysterious project uses AI...</td>\n",
       "      <td>google mysterious project us ai grab programme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1609334783867342856</td>\n",
       "      <td>Does ChatGPT mean robots are coming for skille...</td>\n",
       "      <td>chatgpt mean robot coming skilled job via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1609067407292653573</td>\n",
       "      <td>HIRING: Senior Data Science Consultant / San F...</td>\n",
       "      <td>hiring senior data science consultant san fran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tweet ID                                   Tweet Translated  \\\n",
       "0   1609172529712611329  The Embark Internship has allowed our interns ...   \n",
       "1   1609243867504590850  asked ChatGPT: What jobs will be needed for hu...   \n",
       "2   1609261573184581635  Thanks to ChatGPT, I just learned a new skill!...   \n",
       "3   1609043258625216512  Maybe AI won't be taking our jobs... at least,...   \n",
       "4   1608679261136912384  Well, @openai #ChatGPT completely shit the bed...   \n",
       "5   1608963021824561154  “AI Rewrite: Steve Jobs Stanford Commencement”...   \n",
       "6   1609304139116302339  🧵 How to replace bullshit jobs with AI 🧵 \\n\\n2...   \n",
       "7   1609301436428423174  Four Ways #Jobs Will Respond to #Automation \\n...   \n",
       "8   1608996363123007489  BGSU program to meet critical workforce needs ...   \n",
       "9   1608952318787026947  New books from MIT experts deliver insights on...   \n",
       "10  1609185244145487873  Experts warn Artificial intelligence may repla...   \n",
       "11  1609248074471464961  I explored ChatGPT today and concluded that ou...   \n",
       "12  1609204286981816321  Sophisticated Artificial Intelligence and mach...   \n",
       "13  1609039010642690048  This is priceless. Much debate on whose jobs w...   \n",
       "14  1608935714305069056  RISE OF THE ROBOTS Artificial intelligence can...   \n",
       "15  1609188926295506946  3️⃣#TrendsLLYCDCH: Artificial intelligence cre...   \n",
       "16  1609142479546191872  Automation &amp; Artificial Intelligence is on...   \n",
       "17  1609112876656066560  Twitter !!!\\n\\nlooking for web3 jobs ? \\n\\nTro...   \n",
       "18  1609058620913905664  To save so many Jobs from vanishing we need to...   \n",
       "19  1609232645438701569  So here is the AI Chatbot i.e. ChatGPT Intervi...   \n",
       "20  1609154651441565696  Does ChatGpt mean robots are coming for skille...   \n",
       "21  1609052226986860545  Register now for jobs in Data science, Machine...   \n",
       "22  1608945762536136705  Even though Amazon's automating various busine...   \n",
       "23  1609174530265677825  Onshoring the most complex factories in histor...   \n",
       "24  1609202812046479362  HIRING: Data Management Administrator I / Sain...   \n",
       "25  1609329670285041665  Does ChatGPT mean robots are coming for skille...   \n",
       "26  1609304151137345536  🔗 How One Jobseeker Used AI to Apply for 200 J...   \n",
       "27  1609084395889070080  Now This:  Google’s mysterious project uses AI...   \n",
       "28  1609334783867342856  Does ChatGPT mean robots are coming for skille...   \n",
       "29  1609067407292653573  HIRING: Senior Data Science Consultant / San F...   \n",
       "\n",
       "                                         Text Cleaned  \n",
       "0   embark internship allowed intern grow professi...  \n",
       "1   asked chatgpt job needed human perform artific...  \n",
       "2                    thanks chatgpt learned new skill  \n",
       "3                       maybe ai taking job least yet  \n",
       "4   well completely shit bed one pharmacologist bi...  \n",
       "5   ai rewrite steve job stanford commencement pet...  \n",
       "6   replace bullshit job ai year never seen risk o...  \n",
       "7                                four way respond via  \n",
       "8   bgsu program meet critical workforce need adva...  \n",
       "9   new book mit expert deliver insight future rea...  \n",
       "10  expert warn artificial intelligence may replac...  \n",
       "11          explored chatgpt today concluded job safe  \n",
       "12  sophisticated artificial intelligence machine ...  \n",
       "13  priceless much debate whose job eliminated ai ...  \n",
       "14  rise robot artificial intelligence replace hum...  \n",
       "15  artificial intelligence creates opportunity im...  \n",
       "16  automation amp artificial intelligence one dis...  \n",
       "17  twitter looking web job tropyverse hiring digi...  \n",
       "18  save many job vanishing need ask bjp start boy...  \n",
       "19  ai chatbot e chatgpt interview hand different ...  \n",
       "20              chatgpt mean robot coming skilled job  \n",
       "21  register job data science machine learning art...  \n",
       "22  even though amazon automating various business...  \n",
       "23  onshoring complex factory history require late...  \n",
       "24  hiring data management administrator saint cha...  \n",
       "25          chatgpt mean robot coming skilled job via  \n",
       "26  one jobseeker used ai apply job day ai emergin...  \n",
       "27  google mysterious project us ai grab programme...  \n",
       "28          chatgpt mean robot coming skilled job via  \n",
       "29  hiring senior data science consultant san fran...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text Cleaned\"] = df[\"Tweet Translated\"].apply(preprocessing)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969a6d4",
   "metadata": {},
   "source": [
    "### Labeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b3fc0b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\Desktop\\dicoding_sentimen_analysis\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Labeling data menggunakan Model dari Hugging Face\u001b[39;00m\n\u001b[32m      2\u001b[39m label = [\u001b[33m\"\u001b[39m\u001b[33mPositive\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNeutral\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNegative\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mcls\u001b[39m = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero-shot-classification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/bart-large-mnli\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mText Cleaned\u001b[39m\u001b[33m\"\u001b[39m].head(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Desktop\\dicoding_sentimen_analysis\\env\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m model_config = model.config\n\u001b[32m    953\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Desktop\\dicoding_sentimen_analysis\\env\\Lib\\site-packages\\transformers\\pipelines\\base.py:242\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[33;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[32m    218\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m \u001b[33;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    243\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m     )\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    248\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_from_pipeline\u001b[39m\u001b[33m\"\u001b[39m] = task\n",
      "\u001b[31mRuntimeError\u001b[39m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "# Labeling data menggunakan Model dari Hugging Face\n",
    "label = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "cls = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\", device=0)\n",
    "df[\"Text Cleaned\"].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
